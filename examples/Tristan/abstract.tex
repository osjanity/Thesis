
{
\null\vspace{\fill}
{\center{\section*{Abstract}}}
\addcontentsline{toc}{section}{Abstract}
Modern deep neural networks are becoming deeper, wider, and generally more complex every year. The goal of solving ever more complicated and difficult problems comes with the tradeoff that powerful and expensive hardware is nee\-ded. This is often not possible to employ in mobile and embedded devices. This master thesis is exploring methods of compressing and approximating deep learning models with the objective to make them more accessible on less powerful devices. The discussed techniques to achieve this goal are network quantization, model pruning, and knowledge distillation. Each of those methods approaches the problem in a different way and has other requirements. This leads to a classification and recommendation based on the objectives of the user and the given scenario. A notable contribution is made with a new method of pruning neural network that goes beyond traditional methods and enables better usage of the hardware.

\textbf{Keywords}: machine learning, deep learning, convolutional neural networks, quantization, pruning, knowledge distillation

\vspace{\fill}
}

{
\newpage
\null\vspace{\fill}
{\center{\section*{Zusammenfassung}}}
Moderne tiefe neuronale Netze werden jedes Jahr tiefer, breiter und im Allgemeinen komplexer. Das Ziel immer neue und schwerere Aufgaben zu lösen hat den Preis, dass dafür auch leistungsstärkere und teurere Hardware benötigt wird. Das ist oft nicht möglich in mobilen oder eingebetteten Systemen. Im Rahmen dieser Masterarbeit werden Methoden erforscht, um Modelle für tiefes Lernen zu komprimieren und zu approximieren, mit dem Ziel sie auf weniger leistungsstarken System verfügbar zu machen. Die Techniken dafür, die hier diskutiert werden, sind Quantisierung, Pruning und Knowledge Distillation. Jede dieser Methoden verfolgt einen anderen Ansatz und hat andere Voraussetzungen. Das führt zu einer Klassifikation dieser Methoden anhand der eine Empfeh\-lung ausgesprochen werden kann, ausgehend von der Zielstellung und Situation des Anwenders. Die Arbeit leistet einen eignen Beitrag mit der Einführung einer neuen Methode für Pruning, die über traditionelle Ansätze hinaus geht und bestehende Hardware besser nutzen kann.

\textbf{Schlagwörter}: maschinelles Lernen, tiefes Lernen, faltende neuronale Netze, Quantisierung, Pruning, Knowledge Distillation
\vspace{\fill}
\newpage
}