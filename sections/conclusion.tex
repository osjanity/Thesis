\chapter{Conclusion}
\section{Results and Implications}
\null \quad \quad In this thesis, we have presented preliminaries from graph theory and Bayesian statistics to aid our understanding of Markov equivalence. We defined a query, identified the set $\Delta(G,q)$ upon which a query depends, quantified the cost of the computing an arbitrary query, and determined the speedups attainable for computing a query by reversing an edge in a Bayesian network. \newline
\null \quad \quad We explored major results in Markov equivalence, allowing us to understand the contexts in which edge reversals are possible. This understanding of Markov equivalence let us take advantage of the non-identifiability property of Bayesian networks by developing algorithms which find and reverse edges which reduce the complexity of inference for a specific query. Under reasonable restrictions on graph structure, we quantified the complexity of these algorithms and compared them to the speedups earned through using them. We concluded that the cost of identifying and reversing advantageous edges is almost always worth the speedups gained. \newline
\null \quad \quad Finally, we explored the implications of these results over sequences of queries, and developed tools to help us find optimized permutations of such sequences. We determined that one can develop a tree-structure to describe an optimal traversal of queries by developing a \textit{query distance graph}. \newline
\null \quad \quad \textbf{The major results of this thesis inform us that edge reversal \textit{is}, in most circumstances, an advantageous strategy for reducing the cost of computing a query.} They also give us a context for understanding of how to re-order sequences of queries to minimize cost. Furthermore, the frameworks behind many of these results may allow for others to easily and effectively explore the topics beyond the results presented here.  

\section{Future work} 
\null \quad \quad This work has relied on several restrictions in order to quantify the complexity of the algorithms used. Firstly, many of the scenarios explored were in the binary setting. With some patience, the logic of the binary setting can be extended to categorical variables. Further, in cases where quantifying the complexity of an algorithm is difficult-- say, if the graph has an unrestricted number of parents-- one could experimentally determine whether the greedy strategy is effective by timing it over a large number of randomly generated graphs. Then, even when the arithmetic complexity is unclear, one can gain an understanding of whether it is effective, and if so, under what circumstances. \newline
\null \quad \quad We also explored the greedy strategy, meaning we looked for a single effective edge reversal over the set of covered edges. In future work, one could explore other methods, such as generating the entire Markov equivalence class $[G]$ of a given input DAG $G$, and determining which graph $G' \in [G]$ minimizes the cost of the query. \newline
\null \quad \quad In more detailed consideration, one could compare the method presented in ~\cref{section:bgandconstruction} of determining which case (special or general) a query is in to the other proposed method; that is, compare the method of searching among all paths between target nodes and condition parents to the method of copying the graph, removing the conditional vertex, and checking the structure of the newly created graph.\newline
\null \quad \quad It is likely that the cost of finding beneficial edges to reverse can be reduced by more efficient algorithm design, as well as the space-requirement outlined in Lemma \ref{lem:representspace}.\newline
\null \quad \quad In general, the analysis in this thesis has been limited to only considering the worst-case complexities of the algorithms, as is typical for complexity analysis. However, on average the benefit of these algorithms will be much larger than the complexities presented suggest. Therefore, a valuable line of research would be to explore the more complicated problem of quantifying average-case complexities for each algorithm. Average-case analysis would provide much more insight into the actual benefit of these algorithms in practice. For example, in Theorem \ref{thm:reversalbenefit}, which states that reversing beneficial edges is worthwhile whenever $|V|$ \textgreater \  $22$, the lower bound is built on the assumption of a worst-case scenario. On average, the reversal may be beneficial in a wider set of scenarios. 
